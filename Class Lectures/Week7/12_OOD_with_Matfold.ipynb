{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc74d7fa",
   "metadata": {},
   "source": [
    "# Testing Out-of-Distribution Performance of Materials ML Models with MatFold\n",
    "\n",
    "We will use the same Matminer/Matbench dataset as during lecture 11, but this time we will create an OOD test set using `MatFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets.dataset_retrieval import load_dataset\n",
    "import pandas as pd\n",
    "from MatFold import MatFold\n",
    "\n",
    "df_full = load_dataset('matbench_perovskites')\n",
    "df_full.head()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a74538",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 2000\n",
    "df = df_full.sample(n=2000, random_state=17)  # subsample for speed   # type: ignore\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f4c26",
   "metadata": {},
   "source": [
    "For MatFold, we will need a dictionary of the bulk structures (as a Pymatgen object) and as keys we will create an index that refers to each of them (\"mat0\", \"mat1\",...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34669f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core import Structure\n",
    "\n",
    "df['mat_index'] = ['mat{}'.format(i) for i in range(len(df))]\n",
    "df = df[['mat_index', 'structure', 'e_form']]\n",
    "mat_struct_dict = {\n",
    "    row['mat_index']: row['structure'].as_dict() for _, row in df.iterrows()\n",
    "}\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5654b",
   "metadata": {},
   "source": [
    "We will use the `EwaldSumMatrix` featurizer again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e77e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import EwaldSumMatrix\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum number of atoms across all structures in the dataset\n",
    "n_max = 0\n",
    "for mat in df['structure']:\n",
    "    if len(mat) > n_max :\n",
    "        n_max = len(mat)\n",
    "print(n_max)\n",
    "\n",
    "ews = EwaldSumMatrix(n_atoms_max=n_max, permutation=\"eigenspectrum\")\n",
    "\n",
    "ase_structures = [AseAtomsAdaptor.get_atoms(struc) for struc in df['structure']]\n",
    "ews_matrices = np.array(ews.create(ase_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473191ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ews_columns = [f'ews_{i}' for i in range(ews_matrices.shape[1])]\n",
    "df_featurized_ews = df.copy()\n",
    "df_featurized_ews[ews_columns] = pd.DataFrame(ews_matrices, index=df_featurized_ews.index)\n",
    "df_featurized_ews = df_featurized_ews.drop(columns=['structure'])\n",
    "print(df_featurized_ews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4ef16",
   "metadata": {},
   "source": [
    "MatFold can create out-of-distribution splits based on these categories:\n",
    "\n",
    "- `index` (or `random`)\n",
    "- `structureid` (or `structure`)\n",
    "- `composition` (or `comp`)\n",
    "- `chemsys` (or `chemicalsystem`)\n",
    "- `sgnum` (or `spacegroup`, `spacegroupnumber`)\n",
    "- `pointgroup` (or `pg`, `pointgroupsymbol`, `pgsymbol`)\n",
    "- `crystalsys` (or `crystalsystem`)\n",
    "- `elements` (or `elems`)\n",
    "- `periodictablerows` (or `ptrows`)\n",
    "- `periodictablegroups` (or `ptgroups`)\n",
    "\n",
    "First, we create an instance of the `MatFold` class by supplying the full Pandas dataframe (that contains the features) and the dictionary we created earlier that contains the bulk Pymatgen structures. Then we can use the function `split_statistics` to look at the distributions of different categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38455e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MatFold(df_featurized_ews, mat_struct_dict)\n",
    "mf.split_statistics(\"elements\")\n",
    "mf.split_statistics(\"crystalsys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = mf.create_train_validation_test_splits(\"random\", \"elements\", \n",
    "                                                                   train_fraction=0.7, \n",
    "                                                                   validation_fraction=0.2, \n",
    "                                                                   test_fraction=0.1,\n",
    "                                                                   n_test_min=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a415c",
   "metadata": {},
   "source": [
    "Now, we scale the training dataset features with the `StandardScaler` and then apply that scaling to the validation and test sets. In lecture 11 we scaled the entire dataset at once, which is not ideal because there may be data leakage (information from validation and test set \"leaking\" through the scaling information). For a entirely random split, this effect is minor, but for an OOD test set, this leakage may be more severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_train = train_df['e_form'].to_numpy()\n",
    "y_val = val_df['e_form'].to_numpy()  # type: ignore\n",
    "y_test = test_df['e_form'].to_numpy()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df.loc[:, 'ews_0':'ews_4'])\n",
    "X_val = scaler.transform(val_df.loc[:, 'ews_0':'ews_4'])  # type: ignore\n",
    "X_test = scaler.transform(test_df.loc[:, 'ews_0':'ews_4'])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f2b35",
   "metadata": {},
   "source": [
    "Let's calculate the baseline metrics for both the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafba1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae_test = mean_absolute_error(y_test, [mean_train] * len(y_test))\n",
    "print(f\"Baseline MAE for test-set (predicting mean formation energy): {baseline_mae_test:.4f} eV\")\n",
    "baseline_mae_val = mean_absolute_error(y_val, [mean_train] * len(y_val))  # type: ignore\n",
    "print(f\"Baseline MAE for validation-set (predicting mean formation energy): {baseline_mae_val:.4f} eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548f157",
   "metadata": {},
   "source": [
    "Hyerparameter tuning for the RF model using the validation set metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=17, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))  # type: ignore\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d0929",
   "metadata": {},
   "source": [
    "Final model training and metric evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=17, n_jobs=1)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train)\n",
    "y_val_pred = rf_final.predict(X_val)\n",
    "y_test_pred = rf_final.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val = 0.0\n",
    "max_val = 5.0\n",
    "\n",
    "# Train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Formation Energy')\n",
    "axes[0].set_ylabel('Predicted Formation Energy')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Formation Energy')\n",
    "axes[1].set_ylabel('Predicted Formation Energy')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Formation Energy')\n",
    "axes[2].set_ylabel('Predicted Formation Energy')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33826a9c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The OOD test set metrics are worse than the ID (random) split:\n",
    "\n",
    "ID: MAE=0.377 eV, R2=50.0%\n",
    "\n",
    "OOD: MAE=0.382 eV, R2=30.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc010ad",
   "metadata": {},
   "source": [
    "## Exercise 12.1\n",
    "\n",
    "Now, try this pipeline again but for a different OOD test set category (e.g, `sgnum`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4MSD-Teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
