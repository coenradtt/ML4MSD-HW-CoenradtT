{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b865e453",
   "metadata": {},
   "source": [
    "# Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca154631",
   "metadata": {},
   "source": [
    "## Functions in Python\n",
    "\n",
    "Functions are a very useful construct in Python and are simply defined by the keyword `def`. The `return` keyword specifies what the function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58132ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function with positional (mandatory) argument\n",
    "def square(a):\n",
    "    return a*a\n",
    "print(\"square(2)=\", square(2))\n",
    "\n",
    "\n",
    "# function with keyword (optional) arguments\n",
    "def power(x, n=2):\n",
    "    return x**n\n",
    "print(\"power(2)=\", power(2))  # uses default argument\n",
    "print(\"power(2)=\", power(x=2))  # specifies positional argument with its keyword\n",
    "print(\"power(2, 3)=\", power(2, 3))  # explicitly passes both arguments\n",
    "print(\"power(2, n = 3)=\", power(2, n=3)) # explicitly passes the argument with keyword name\n",
    "print(\"power(2, n = 3)=\", power(x=2, n=3)) # explicitly passes both argument with keyword name\n",
    "\n",
    "\n",
    "# lambda function (inline function)\n",
    "lambda_square = lambda x, y: x**2 + y**2  \n",
    "# lambda <variable_name1>, <variable_name2>: <some expression using the variables that should be computed and returned>\n",
    "print(\"lambda_square(2)=\", lambda_square(2, 3))\n",
    "\n",
    "\n",
    "# function with type hinting\n",
    "def add(a: int, b: int, c: int = 3) -> int:\n",
    "    return a + b + c\n",
    "print(\"add(1, 2, 3)=\", add(1, 2))\n",
    "\n",
    "# type hint lambda function\n",
    "from typing import Callable\n",
    "lambda_power: Callable[[float, int], float] = lambda x, y: x**y\n",
    "\n",
    "\n",
    "# variable number of positional arguments\n",
    "def return_args(var, *args):\n",
    "    return var, args  # args will be a tuple\n",
    "\n",
    "var1, rest1 = return_args(1, 2, 3)\n",
    "print(f\"{var1=}, {rest1=}\")\n",
    "var2, rest2 = return_args(2, \"this returns\", 4, \"arguments\")\n",
    "print(f\"{var2=}, {rest2=}\")\n",
    "\n",
    "### keyword arguments\n",
    "def return_kwargs(var, opt_var=5.0, **kwargs):\n",
    "    return var, opt_var, kwargs  #kwargs will be a dictionary\n",
    "\n",
    "var1, opt_var1, rest1 = return_kwargs(1, c=3)\n",
    "print(f\"{var1=}, {opt_var1=}, {rest1=}\")\n",
    "var2, opt_var2, rest2 = return_kwargs(1, c=4, d=\"arguments\")\n",
    "print(f\"{var2=}, {opt_var2=}, {rest2=}\")\n",
    "\n",
    "# `return` keyword exits the function, any code after `return` is reached will not run.\n",
    "def odd_or_even(val):\n",
    "    if val % 2 == 0:\n",
    "        return \"even\"\n",
    "    else:\n",
    "        return \"odd\"\n",
    "\n",
    "def odd_or_even_shorter(val):\n",
    "    if val % 2 == 0:\n",
    "        return \"even\"  # return exits the function\n",
    "    return \"odd\"\n",
    "\n",
    "# You can force arguments to be position-only\n",
    "def f(a, b, /, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "f(1, 2, 3)  # works\n",
    "f(1, 2, c=3)  # works\n",
    "# f(a=1, b=2, c=3)  # Error\n",
    "\n",
    "# You can force arguments to be keyword-only\n",
    "def g(a, *, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "g(1, b=2, c=3)  # works\n",
    "# g(1, 2, 3)  # Error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbf7a1",
   "metadata": {},
   "source": [
    "### Exercise 5.1\n",
    "\n",
    "Write a function `estimate_effective_mass` that takes:  \n",
    "\n",
    "- Three **positional arguments**:  \n",
    "  - `band_gap` (float, in eV)  \n",
    "  - `electron_mass` (float, in units of free electron mass)  \n",
    "  - `hole_mass` (float, in units of free electron mass)  \n",
    "\n",
    "- One **keyword argument**:  \n",
    "  - `method` (string, `\"average\"` or `\"reduced\"`) — determines how to combine the masses  (defaults to `\"\"average\"`)\n",
    "\n",
    "Inside the function:  \n",
    "- If `method == \"average\"`, return the arithmetic mean of `electron_mass` and `hole_mass`.  \n",
    "- If `method == \"reduced\"`, return the reduced mass:  \n",
    "\n",
    "\\[\n",
    "m^* = \\frac{m_e \\cdot m_h}{m_e + m_h}\n",
    "\\]\n",
    "\n",
    "- If `band_gap < 0.1 eV`, print a warning `\"Material may be metallic!\"` before returning.  \n",
    "\n",
    "**Example usage:**  \n",
    "```python\n",
    "print(estimate_effective_mass(1.1, 0.98, 0.81, method=\"average\"))\n",
    "print(estimate_effective_mass(0.3, 1.2, 0.9, method=\"reduced\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe6a4e",
   "metadata": {},
   "source": [
    "## Gradient Descent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39098905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# function to minimize\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# derivative of the function\n",
    "def d_f(x):\n",
    "    return 2*x\n",
    "\n",
    "# gradient descent algorithm\n",
    "def gradient_descent(x0, learning_rate, steps):\n",
    "    x = x0\n",
    "    x_values = [x0]\n",
    "    for i in range(steps):\n",
    "        x -= learning_rate * d_f(x)\n",
    "        x_values.append(x)\n",
    "    return x_values\n",
    "\n",
    "# initialize parameters\n",
    "\n",
    "# initial value\n",
    "x0 = -10\n",
    "steps = 20\n",
    "learning_rates = [0.01, 0.1, 1]\n",
    "# learning rates to plot\n",
    "\n",
    "# initialize the figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# colors for the plots\n",
    "colors = ['C0', 'C1', 'C2']\n",
    "\n",
    "# plot the gradient descent for different learning rates\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    # compute gradient descent\n",
    "    x_values = gradient_descent(x0, learning_rate, steps)\n",
    "\n",
    "    # function values at gradient descent steps\n",
    "    y_values = [f(x_val) for x_val in x_values]\n",
    "\n",
    "    # plot the original function\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = f(x)\n",
    "    axs[i].plot(x, y, 'k--', label='f(x)', alpha=0.7)\n",
    "\n",
    "    # plot the gradient descent\n",
    "    axs[i].scatter(x_values, y_values, label=f'learning_rate={learning_rate}', c=colors[i])\n",
    "\n",
    "    # plot the final value of function after gradient descent steps\n",
    "    axs[i].scatter(x_values[-1], f(x_values[-1]), c=colors[i], s=150, marker='x')\n",
    "\n",
    "    axs[i].legend()\n",
    "    # axs[i].grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
    "    axs[i].set_xlabel('x')\n",
    "    axs[i].set_ylabel('f(x)')\n",
    "    axs[i].set_title(f'Learning Rate: {learning_rate}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34391e",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ffec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 2 * np.pi, 30)\n",
    "y = np.sin(X) + 0.2 * np.random.randn(len(X))  # Targeted true function: sin(x) but we add noise\n",
    "\n",
    "# Train/test split (70% train, 30% test)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.7 * len(X))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Scaling the input features (after scaling -> mean ≈ 0, variance ≈ 1): Improves numeric robustness\n",
    "def scale(X):\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    return (X - mean) / std\n",
    "\n",
    "X_train_scaled = scale(X_train)\n",
    "X_test_scaled = scale(X_test)\n",
    "X_full_scaled = scale(X)  # for visualization\n",
    "\n",
    "# Polynomial fitting functions\n",
    "def fit_polynomial(X, y, degree):\n",
    "    X_poly = np.vander(X, degree + 1, increasing=True)\n",
    "    coeffs = np.linalg.pinv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "    return coeffs\n",
    "\n",
    "def predict_polynomial(X, coeffs):\n",
    "    X_poly = np.vander(X, len(coeffs), increasing=True)\n",
    "    return X_poly @ coeffs\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Plotting Fits\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train, y_train, color=\"blue\", label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, color=\"black\", marker=\"x\", label=\"Test data\")\n",
    "\n",
    "X_grid = np.linspace(X.min(), X.max(), 400)\n",
    "X_grid_scaled = scale(X_grid)\n",
    "\n",
    "for deg, col, lab in zip([1, 4, 13], [\"red\", \"green\", \"orange\"], \n",
    "                         [\"Underfit (deg=1)\", \"Good fit (deg=4)\", \"Overfit (deg=13)\"]):\n",
    "    coeffs = fit_polynomial(X_train_scaled, y_train, deg)\n",
    "    y_grid = predict_polynomial(X_grid_scaled, coeffs)\n",
    "    plt.plot(X_grid, y_grid, color=col, label=lab)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Fits with Scaled Features\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef040cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 14\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "for deg in range(1, max_degree + 1):\n",
    "    coeffs = fit_polynomial(X_train_scaled, y_train, deg)\n",
    "    \n",
    "    y_pred_train = predict_polynomial(X_train_scaled, coeffs)\n",
    "    y_pred_test = predict_polynomial(X_test_scaled, coeffs)\n",
    "    \n",
    "    train_errors.append(mse(y_train, y_pred_train))\n",
    "    test_errors.append(mse(y_test, y_pred_test))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_degree + 1), train_errors, marker=\"o\", label=\"Train Error\")\n",
    "plt.plot(range(1, max_degree + 1), test_errors, marker=\"s\", label=\"Test Error\")\n",
    "plt.axvline(4, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Good fit (deg≈4)\")\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Train vs Test Error (Bias-Variance Tradeoff, Scaled X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4MSD-Teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
