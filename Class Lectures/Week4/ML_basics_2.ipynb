{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b865e453",
   "metadata": {},
   "source": [
    "# Machine Learning Basics 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34391e",
   "metadata": {},
   "source": [
    "## Regularization: Ridge Regression Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 2 * np.pi, 30)\n",
    "y = np.sin(X) + 0.2 * np.random.randn(len(X))  # Targeted true function: sin(x) but we add noise\n",
    "\n",
    "# Train/test split (70% train, 30% test)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.7 * len(X))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Scaling the input features (after scaling -> mean ≈ 0, variance ≈ 1): Improves numeric robustness\n",
    "def scale(X_tr, X_te):\n",
    "    mean = X_tr.mean(axis=0)\n",
    "    std = X_tr.std(axis=0)\n",
    "    return (X_tr - mean) / std, (X_te - mean) / std\n",
    "\n",
    "# Polynomial fitting functions\n",
    "def ridge_regression(X, y, degree, lam):\n",
    "    X_poly = np.vander(X, degree + 1, increasing=True)\n",
    "    D = np.eye(degree + 1)\n",
    "    D[0, 0] = 0  # do not penalize the bias term\n",
    "    return np.linalg.pinv(X_poly.T @ X_poly + lam * D) @ X_poly.T @ y\n",
    "\n",
    "def model_prediction(X, coeffs):\n",
    "    X_poly = np.vander(X, len(coeffs), increasing=True)\n",
    "    return X_poly @ coeffs\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale(X_train, X_test)\n",
    "degree = 20\n",
    "\n",
    "# Try different lambda values\n",
    "lambdas = [1e-2, 1e-1, 1, 5, 10, 100]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, lam in enumerate(lambdas, 1):\n",
    "    coeffs = ridge_regression(X_train_scaled, y_train, degree, lam)\n",
    "    y_test_pred = model_prediction(X_test_scaled, coeffs)\n",
    "    order = np.argsort(X_test)\n",
    "\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.plot(X_test[order], y_test_pred[order], color=\"r\", label=\"Prediction\")\n",
    "    plt.scatter(X_train, y_train, color=\"blue\", label=\"Train data\")\n",
    "    plt.scatter(X_test, y_test, color=\"black\", marker=\"x\", label=\"Test data\")\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.title(f\"λ={lam}\")\n",
    "    if i == 1:\n",
    "        plt.legend()\n",
    "\n",
    "plt.suptitle(f\"Ridge Regression with Polynomial Degree {degree}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "lambda_grid = np.logspace(-2, 2, 50)\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "for lam in lambda_grid:\n",
    "    coeffs = ridge_regression(X_train_scaled, y_train, degree, lam)\n",
    "    y_train_pred = model_prediction(X_train_scaled, coeffs)\n",
    "    y_test_pred = model_prediction(X_test_scaled, coeffs)\n",
    "    train_errors.append(mse(y_train, y_train_pred))\n",
    "    test_errors.append(mse(y_test, y_test_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(lambda_grid, train_errors, label=\"Train error\")\n",
    "plt.semilogx(lambda_grid, test_errors, label=\"Test error\")\n",
    "plt.xlabel(\"λ (regularization strength)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Effect of Regularization on Generalization\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28aa972",
   "metadata": {},
   "source": [
    "## Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfde67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate toy data (two Gaussian blobs)\n",
    "N = 100\n",
    "X_class0 = np.random.randn(N, 2) + np.array([-1, -1])\n",
    "X_class1 = np.random.randn(N, 2) + np.array([1, 1])\n",
    "\n",
    "X = np.vstack([X_class0, X_class1])\n",
    "y = np.array([0]*N + [1]*N)\n",
    "\n",
    "# Add bias column (intercept term)\n",
    "X_bias = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Functions:\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict_prob(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict_class(X, theta):\n",
    "    return (predict_prob(X, theta) >= 0.5).astype(int)\n",
    "\n",
    "def loss(X, y, theta):\n",
    "    h = predict_prob(X, theta)\n",
    "    return -np.mean(y*np.log(h+1e-12) + (1-y)*np.log(1-h+1e-12))\n",
    "\n",
    "# Gradient descent\n",
    "def logistic_regression(X, y, lr=0.1, epochs=1000):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        h = predict_prob(X, theta)\n",
    "        gradient = X.T @ (h - y) / len(y)\n",
    "        theta -= lr * gradient\n",
    "        losses.append(loss(X, y, theta))\n",
    "    return theta, losses\n",
    "\n",
    "theta, losses = logistic_regression(X_bias, y, lr=0.1, epochs=1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Decision boundary and data\n",
    "axes[0].scatter(X_class0[:,0], X_class0[:,1], color=\"blue\", label=\"Class 0\")\n",
    "axes[0].scatter(X_class1[:,0], X_class1[:,1], color=\"red\", label=\"Class 1\")\n",
    "\n",
    "x1 = np.linspace(-5, 5, 100)\n",
    "x2 = -(theta[0] + theta[1]*x1) / theta[2]  # line: θ0 + θ1*x1 + θ2*x2 = 0\n",
    "axes[0].plot(x1, x2, \"k--\", label=\"Decision boundary\")\n",
    "\n",
    "axes[0].set_xlim([-4, 4])\n",
    "axes[0].set_ylim([-4, 4])\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "axes[0].set_title(\"Logistic Regression (from scratch)\")\n",
    "\n",
    "# Training loss for gradient descent\n",
    "axes[1].plot(losses, color=\"purple\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e75d24",
   "metadata": {},
   "source": [
    "## Under/overfitting with Polynomial Logistic Regression (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9525ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "np.random.seed(0)\n",
    "N = 300\n",
    "X = np.random.uniform(-3, 3, (N, 2))\n",
    "\n",
    "# True decision boundary: y = x^2 - 1\n",
    "y_true = X[:,1] > (X[:,0]**2 - 1)\n",
    "y = y_true.astype(int)\n",
    "\n",
    "# Add noise (flip some labels randomly to make non-separable)\n",
    "dist_to_boundary = np.abs(X[:,1] - (X[:,0]**2 - 1))\n",
    "flip_prob = 0.5 * np.exp(-dist_to_boundary**2 / 0.5)  # higher prob near boundary\n",
    "random_vals = np.random.rand(N)\n",
    "flip_mask = random_vals < flip_prob\n",
    "y[flip_mask] = 1 - y[flip_mask]\n",
    "\n",
    "# Functions:\n",
    "def polynomial_features(X, degree):\n",
    "    \"\"\"Return polynomial features up to given degree.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    features = [np.ones(n_samples)]\n",
    "    for d in range(1, degree+1):\n",
    "        for comb in combinations_with_replacement(range(n_features), d):\n",
    "            features.append(np.prod(X[:, comb], axis=1))\n",
    "    return np.vstack(features).T\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict_prob(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict_class(X, theta):\n",
    "    return (predict_prob(X, theta) >= 0.5).astype(int)\n",
    "\n",
    "def loss(X, y, theta):\n",
    "    h = predict_prob(X, theta)\n",
    "    return -np.mean(y*np.log(h+1e-12) + (1-y)*np.log(1-h+1e-12))\n",
    "\n",
    "def logistic_regression(X, y, lr=0.1, epochs=1000):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(epochs):\n",
    "        h = predict_prob(X, theta)\n",
    "        gradient = X.T @ (h - y) / len(y)\n",
    "        theta -= lr * gradient\n",
    "    return theta\n",
    "\n",
    "# Different degrees of polynomials (higher -> more complex)\n",
    "degrees = [1, 2, 6]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, degree in enumerate(degrees, 1):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    theta = logistic_regression(X_poly, y, lr=0.1, epochs=3000)\n",
    "    \n",
    "    # Decision boundary grid\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-3, 3, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_poly = polynomial_features(grid, degree)\n",
    "    Z = predict_class(grid_poly, theta).reshape(xx.shape)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], color=\"blue\", s=10)\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", s=10)\n",
    "    plt.title(f\"Polynomial Degree {degree}\")\n",
    "    plt.xlim(-3, 3); plt.ylim(-3, 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ede01",
   "metadata": {},
   "source": [
    "## K-Means Clustering Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Three clusters in 2D\n",
    "np.random.seed(42)\n",
    "X1 = np.random.randn(50, 2) + np.array([0, 0])\n",
    "X2 = np.random.randn(50, 2) + np.array([5, 5])\n",
    "X3 = np.random.randn(50, 2) + np.array([0, 5])\n",
    "X = np.vstack([X1, X2, X3])\n",
    "\n",
    "def kmeans(X, K=3, max_iters=6):\n",
    "    rng = np.random.default_rng(17)\n",
    "    centroids = X[rng.choice(len(X), K, replace=False)]  # Random initialization of centroids\n",
    "    history = [(centroids.copy(), None)]  # Iteration 0 (no labels yet)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Step 1: Assign points to nearest centroid\n",
    "        distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Save state for plotting\n",
    "        history.append((centroids.copy(), labels.copy()))\n",
    "\n",
    "        # Step 2: Update centroids\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n",
    "        if np.allclose(centroids, new_centroids, rtol=1e-3):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Compute inertia (sum of squared distances to nearest centroid)\n",
    "    inertia = np.sum((X - centroids[labels])**2)\n",
    "\n",
    "    return centroids, labels, history, inertia\n",
    "\n",
    "final_centroids, final_labels, history, _ = kmeans(X, K=3, max_iters=10)\n",
    "\n",
    "# --- Plot iterations (max 2 per row) ---\n",
    "n_iter = len(history)\n",
    "n_cols = 2\n",
    "n_rows = int(np.ceil(n_iter / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (centroids, labels) in enumerate(history):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot points\n",
    "    if labels is None:  # Iteration 0\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=\"gray\", s=20, alpha=0.5, label=\"Data points\")\n",
    "    else:\n",
    "        for k in range(3):\n",
    "            ax.scatter(X[labels == k, 0], X[labels == k, 1], s=20, label=f\"Cluster {k+1}\")\n",
    "\n",
    "    # Plot centroids\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c=\"black\", s=120, marker=\"X\", label=\"Centroids\")\n",
    "\n",
    "    # Draw arrows showing centroid movement (skip Iteration 1)\n",
    "    if i > 1:\n",
    "        prev_centroids, _ = history[i-1]\n",
    "        for (x_old, y_old), (x_new, y_new) in zip(prev_centroids, centroids):\n",
    "            ax.arrow(x_old, y_old,\n",
    "                     x_new - x_old, y_new - y_old,\n",
    "                     head_width=0.2, head_length=0.2, fc=\"purple\", ec=\"purple\")\n",
    "\n",
    "    ax.set_title(f\"Iteration {i}\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"K-means clustering iterations with centroid movement\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577b74f",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "Plot inertia vs. number of clusters K\n",
    "\n",
    "You get the intertia with the above function, for example for K=5:\n",
    "\n",
    "`_, _, _, inertia = kmeans(X, K=5, max_iters=20`)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9807ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e9931",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA) Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a916ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlated data in 2D:\n",
    "np.random.seed(0)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2], [2, 2]]  # correlated features\n",
    "X = np.random.multivariate_normal(mean, cov, size=200)\n",
    "\n",
    "# Step 1: Center the data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Step 3: Eigen decomposition\n",
    "eigvals, eigvecs = np.linalg.eigh(cov_matrix)  # symmetric\n",
    "order = np.argsort(eigvals)[::-1]\n",
    "eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "\n",
    "# Step 4: Project data into PCA coordinates\n",
    "X_pca = X_centered @ eigvecs\n",
    "\n",
    "# Explained variance ratio:\n",
    "explained_var_ratio = eigvals / eigvals.sum()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.4)\n",
    "origin = np.mean(X_centered, axis=0)\n",
    "for val, vec in zip(eigvals, eigvecs.T):\n",
    "    plt.quiver(*origin, *(vec * np.sqrt(val) * 2),\n",
    "               angles='xy', scale_units='xy', scale=1, color='red', width=0.005)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(\"Original data with principal axes\")\n",
    "plt.xlabel(\"x₁\")\n",
    "plt.ylabel(\"x₂\")\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Plotting projection onto first principle axis\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], np.zeros_like(X_pca[:, 0]), alpha=0.5, c=\"blue\")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(f\"Projection onto 1st PC (Expl. Var. = {explained_var_ratio[0]:.2f})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.yticks([])  # hide y-axis since it's 1D\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4MSD-Teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
