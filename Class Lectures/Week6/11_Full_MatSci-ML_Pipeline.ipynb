{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fbfb95",
   "metadata": {},
   "source": [
    "# Demonstration of Full ML Pipeline with a Materials Science Database\n",
    "\n",
    "We will use Matminer to fetch materials datasets that are ML-ready (i.e., they largely don't require any data cleaning step). We will then consider two featurization approaches: compositional (using Matminer again) and structural (using DScribe). Then we will remove low variance features and highly correlated features with Pandas. Lastly, we use Scikit-Learn for data splitting, recursive feature elimintation, model performance metrics calculations, and ML model training (random forest model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets.dataset_retrieval import load_dataset, get_all_dataset_info\n",
    "from matminer.datasets import get_available_datasets\n",
    "\n",
    "print(*get_available_datasets(print_format='short'), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed5331",
   "metadata": {},
   "source": [
    "For this demonstration let us work with the Matbench perovskite dataset which contains 18,928 DFT-calculated formation energies (in eV/unit cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_all_dataset_info('matbench_perovskites'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf4e2d",
   "metadata": {},
   "source": [
    "## 1. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e986c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_full = load_dataset('matbench_perovskites')\n",
    "df_full.describe()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cb5e5",
   "metadata": {},
   "source": [
    "For computational efficiency sake, we will randomly sample a smaller number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 2000\n",
    "df = df_full.sample(n=2000, random_state=17)  # subsample for speed   # type: ignore\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed4b67",
   "metadata": {},
   "source": [
    "Now, let us have a look at the distribution of the larget property `e_form`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['e_form'].hist(bins=30)\n",
    "plt.xlabel('Formation Energy (eV/unit cell)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Formation Energies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06272c47",
   "metadata": {},
   "source": [
    "## 2. Composition-based Features\n",
    "First, let us add a new column that contains the composition of each entry through converting the Pymatgen structure to a composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94537c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df.copy()\n",
    "df_comp['composition'] = df_comp.structure.apply(lambda x: x.composition )\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95534987",
   "metadata": {},
   "source": [
    "### 2.1 Featurization\n",
    "Then we will featurize the compositions using the Matminer `ElementProperty` featurizer and use the magpie preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.featurizers.composition.composite import ElementProperty\n",
    "\n",
    "el_prop_featuriser = ElementProperty.from_preset(preset_name='magpie', impute_nan=False)\n",
    "el_prop_featuriser.set_n_jobs(1)\n",
    "df_featurized = el_prop_featuriser.featurize_dataframe(df_comp, col_id='composition')\n",
    "\n",
    "print(df_featurized.shape)  # type: ignore\n",
    "print(df_featurized.isnull().sum().sum())  # Check for any NaN values  # type: ignore\n",
    "df_featurized.head()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439fefc",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering and Cleaning\n",
    "We store the target column is `\"e_form\"` in variable `y`, and the features in `X_all`.\n",
    "\n",
    "We remove all features with very small variance and features that highly correlate with other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99585dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_featurized['e_form']  # type: ignore\n",
    "X_all = df_featurized.drop(columns=['e_form', 'structure', 'composition'])  # type: ignore\n",
    "\n",
    "print(\"Number of features before cleaning:\", X_all.shape[1])\n",
    "\n",
    "# Identify columns with very small variance and drop them\n",
    "small_var_cols = X_all.columns[X_all.var() < 1e-5].tolist()\n",
    "print(\"Columns with very small variance:\", small_var_cols)\n",
    "X_all = X_all.drop(columns=small_var_cols)\n",
    "corr_matrix = X_all.corr(method='pearson')\n",
    "print(\"Number of features after removing small variance columns:\", X_all.shape[1])\n",
    "\n",
    "# Remove highly correlated columns\n",
    "threshold = 0.99\n",
    "to_drop = set()\n",
    "for col in corr_matrix.columns:\n",
    "    high_corr = corr_matrix.index[(corr_matrix[col].abs() > threshold) & (corr_matrix.index != col)]\n",
    "    to_drop.update(high_corr)\n",
    "print(\"Columns to drop due to high correlation:\", to_drop)\n",
    "X = X_all.drop(columns=list(to_drop))\n",
    "print(\"Number of features after removing highly correlated columns:\", X.shape[1])\n",
    "\n",
    "# For the remaining features, let's visualize the correlation matrix\n",
    "corr_matrix = X.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "im = plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns, fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dec31",
   "metadata": {},
   "source": [
    "### 2.3 Feature Scaling and Dataset Splitting\n",
    "Now we use the `StandardScaler` of the Scikit-Learn package, which is always advisable for all input features as it improves numeric stability during model optimization (e.g., gradient descent).\n",
    "\n",
    "Then we split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "test_fraction = 0.1\n",
    "validation_fraction = 0.2\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled, y, \n",
    "                                                          test_size=test_fraction, \n",
    "                                                          random_state=17)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, \n",
    "                                                  test_size=validation_fraction/(1-test_fraction), \n",
    "                                                  random_state=17)\n",
    "print(f\"Training fraction: {X_train.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Validation fraction: {X_val.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Test fraction: {X_test.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ca14d",
   "metadata": {},
   "source": [
    "### 2.4 Dummy Baseline Model\n",
    "\n",
    "We will compute the MAE for a dummy baseline model, which always predicts the average of the training set target value for all validation predictions. This is used as a reference to compare how well the model learns compared to a very naive \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_val, [mean_train] * len(y_val))\n",
    "\n",
    "print(f\"Baseline MAE (predicting mean formation energy): {baseline_mae:.4f} eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c44375",
   "metadata": {},
   "source": [
    "### 2.5 Hyperparameter Optimization\n",
    "\n",
    "Here, we will choose the `RandomForestRegressor` of Scikit-Learn which has a few hyperparameters. We will optimize only one here for demonstration purposes: `n_estimators`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b64aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=17, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbe267",
   "metadata": {},
   "source": [
    "### 2.6 Recursive Feature Elimintation\n",
    "\n",
    "Now we use the RF model to further downselect important features by using the `RFE` class of Scikit-Learn.\n",
    "\n",
    "*Remark*: This step can get quite compute-intensive! Here, we check RFE in steps of 5 features. Ideally, this should be carried out in steps of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=17, n_jobs=1)\n",
    "n_features_list = list(range(5, X_val.shape[1]+1, 5))\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "selected_features_dict = {}\n",
    "\n",
    "for n_features in tqdm(n_features_list, desc='RFE Progress'):\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features, step=5)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features_dict[n_features] = list(X.columns[rfe.support_])\n",
    "    X_train_rfe = rfe.transform(X_train)\n",
    "    rf.fit(X_train_rfe, y_train)\n",
    "    y_train_pred = rf.predict(X_train_rfe)\n",
    "    train_errors.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    X_val_rfe = rfe.transform(X_val)\n",
    "    y_val_pred = rf.predict(X_val_rfe)\n",
    "    val_errors.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_features_list, train_errors, label='Train MAE')\n",
    "plt.plot(n_features_list, val_errors, label='Validation MAE')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('RFE: MAE vs. Number of Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2b01e",
   "metadata": {},
   "source": [
    "### 2.7 Final Model Evaluation\n",
    "Lastly, we compute all train/validation/test metrics (here we look at MAE and R^2, but this can be easily extended to RMSE, MAPE, etc.). We also create parity plots (true vs. predicted target values) for all three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38061e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "final_features = selected_features_dict[30]\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=17, n_jobs=1)\n",
    "X_train_final = X_train[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "rf_final.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict on train, validation, and test sets\n",
    "X_val_final = X_val[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "X_test_final = X_test[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train_final)\n",
    "y_val_pred = rf_final.predict(X_val_final)\n",
    "y_test_pred = rf_final.predict(X_test_final)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val = 0.0\n",
    "max_val = 5.0\n",
    "\n",
    "# Train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Formation Energy')\n",
    "axes[0].set_ylabel('Predicted Formation Energy')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Formation Energy')\n",
    "axes[1].set_ylabel('Predicted Formation Energy')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Formation Energy')\n",
    "axes[2].set_ylabel('Predicted Formation Energy')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc8234",
   "metadata": {},
   "source": [
    "## 3. Structure-based Features\n",
    "\n",
    "Now, let us use the same steps but instead of using compositional featurization, we will use a strucutral one (the Ewald sum matrix approach via the DScribe package)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f4f1c",
   "metadata": {},
   "source": [
    "### 3.1 Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297218cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import EwaldSumMatrix\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum number of atoms across all structures in the dataset\n",
    "n_max = 0\n",
    "for mat in df['structure']:\n",
    "    if len(mat) > n_max :\n",
    "        n_max = len(mat)\n",
    "print(n_max)\n",
    "\n",
    "ews = EwaldSumMatrix(n_atoms_max=n_max, permutation=\"eigenspectrum\")\n",
    "\n",
    "ase_structures = [AseAtomsAdaptor.get_atoms(struc) for struc in df['structure']]\n",
    "ews_matrices = np.array(ews.create(ase_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ews_columns = [f'ews_{i}' for i in range(ews_matrices.shape[1])]\n",
    "df_featurized_ews = df.copy()\n",
    "df_featurized_ews[ews_columns] = pd.DataFrame(ews_matrices, index=df_featurized_ews.index)\n",
    "print(df_featurized_ews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2c7b1",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f556ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_featurized_ews['e_form']\n",
    "X_all_struc = df_featurized_ews.drop(columns=['e_form', 'structure'])\n",
    "\n",
    "print(\"Number of features before cleaning:\", X_all_struc.shape[1])\n",
    "\n",
    "# Identify columns with very small variance and drop them\n",
    "small_var_cols_struc = X_all_struc.columns[X_all_struc.var() < 1e-5].tolist()\n",
    "print(\"Columns with very small variance:\", small_var_cols_struc)\n",
    "X_all_struc = X_all_struc.drop(columns=small_var_cols_struc)\n",
    "corr_matrix_struc = X_all_struc.corr(method='pearson')\n",
    "print(\"Number of features after removing small variance columns:\", X_all_struc.shape[1])\n",
    "\n",
    "# Remove highly correlated columns\n",
    "threshold = 0.99\n",
    "to_drop_struc = set()\n",
    "for col in corr_matrix_struc.columns:\n",
    "    high_corr_struc = corr_matrix_struc.index[(corr_matrix_struc[col].abs() > threshold) & (corr_matrix_struc.index != col)]\n",
    "    to_drop_struc.update(high_corr_struc)\n",
    "print(\"Columns to drop due to high correlation:\", to_drop_struc)\n",
    "X_struc = X_all_struc.drop(columns=list(to_drop_struc))\n",
    "print(\"Number of features after removing highly correlated columns:\", X_struc.shape[1])\n",
    "\n",
    "corr_matrix_struc = X_struc.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "im = plt.imshow(corr_matrix_struc, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.xticks(range(len(corr_matrix_struc.columns)), [str(c) for c in corr_matrix_struc.columns], rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(corr_matrix_struc.columns)), [str(c) for c in corr_matrix_struc.columns], fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f03f97",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling and Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9037f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_struc)\n",
    "\n",
    "test_fraction = 0.1\n",
    "validation_fraction = 0.2\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled, y, \n",
    "                                                          test_size=test_fraction, \n",
    "                                                          random_state=17)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, \n",
    "                                                  test_size=validation_fraction/(1-test_fraction), \n",
    "                                                  random_state=17)\n",
    "print(f\"Training fraction: {X_train.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Validation fraction: {X_val.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Test fraction: {X_test.shape[0] / X_scaled.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e6c21",
   "metadata": {},
   "source": [
    "### 3.4 Dummy Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead44398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_val, [mean_train] * len(y_val))\n",
    "print(f\"Baseline MAE (predicting mean formation energy): {baseline_mae:.4f} eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1b0d6",
   "metadata": {},
   "source": [
    "### 3.5 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=17, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910f61d",
   "metadata": {},
   "source": [
    "### 2.6 Recursive Feature Elimintation\n",
    "Technically, this step is not required because we already start off with a low number of features to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=17, n_jobs=1)\n",
    "n_features_list = list(range(1, X_val.shape[1]+1, 1))\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "selected_features_dict = {}\n",
    "\n",
    "for n_features in tqdm(n_features_list, desc='RFE Progress'):\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features, step=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features_dict[n_features] = list(X_struc.columns[rfe.support_])\n",
    "    X_train_rfe = rfe.transform(X_train)\n",
    "    rf.fit(X_train_rfe, y_train)\n",
    "    y_train_pred = rf.predict(X_train_rfe)\n",
    "    train_errors.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    X_val_rfe = rfe.transform(X_val)\n",
    "    y_val_pred = rf.predict(X_val_rfe)\n",
    "    val_errors.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_features_list, train_errors, label='Train MAE')\n",
    "plt.plot(n_features_list, val_errors, label='Validation MAE')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('RFE: MAE vs. Number of Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819098b",
   "metadata": {},
   "source": [
    "### 3.7 Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=17, n_jobs=1)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train)\n",
    "y_val_pred = rf_final.predict(X_val)\n",
    "y_test_pred = rf_final.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val = 0.0\n",
    "max_val = 5.0\n",
    "\n",
    "# Train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Formation Energy')\n",
    "axes[0].set_ylabel('Predicted Formation Energy')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Formation Energy')\n",
    "axes[1].set_ylabel('Predicted Formation Energy')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Formation Energy')\n",
    "axes[2].set_ylabel('Predicted Formation Energy')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958afe32",
   "metadata": {},
   "source": [
    "**Conclusion**: 5 structural features outperformed 100+ compositional features! \n",
    "\n",
    "There is still for improvement though. Top compositional features could be added to the 5 structural features. Alternatively, more sophisticated approaches (graph-based) may perform better here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db68080",
   "metadata": {},
   "source": [
    "## 4. Exercise 11.1 → Homework 4\n",
    "\n",
    "Now, it's your turn to utilize this entire pipeline on a different Matminer dataset! However, notice that some datasets only have compositions available and not the structures. Do it twice, once for a compositional featurization and once for a structural one (or, if your dataset does not contain structures, use a second compositional featurization approach).\n",
    "\n",
    "Please, fill in the details in this [Google Sheet](https://docs.google.com/spreadsheets/d/1xbT4lRMYQGrhBQGFczFD5wscrriwWQGf82Gv9AnDkbA/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31037ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available datasets:\n",
    "from matminer.datasets import get_available_datasets\n",
    "\n",
    "print(*get_available_datasets(print_format='short'), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903ec6a",
   "metadata": {},
   "source": [
    "List of available Matminer datasets: [link](https://hackingmaterials.lbl.gov/matminer/dataset_summary.html)\n",
    "\n",
    "List of available Matminer featurizers: [link](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html)\n",
    "\n",
    "List of available DScribe featurizers: [link](https://singroup.github.io/dscribe/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4MSD-Teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
